#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bilibiliè§†é¢‘å­—å¹•ä¸‹è½½ä¸æ€»ç»“ - WebæœåŠ¡
"""

import os
import sys
import json
import uuid
import time
import threading
import shutil
from pathlib import Path
from datetime import datetime
from queue import Queue
from flask import Flask, render_template, request, jsonify, send_from_directory
from werkzeug.utils import secure_filename

from process_generated_content import save_data_to_excel
from bilibili_subtitle_downloader import BilibiliSubtitleDownloader, load_cookies_from_file
from process_video_info import sanitize_filename
from subtitle_summarizer import SRTParser, SubtitleSummarizer, load_llm_config
from llm_client import OpenAICompatClient
from define import create_empty_course

# ç¡®å®šæ¨¡æ¿ç›®å½•
if getattr(sys, 'frozen', False):
    # å¦‚æœæ˜¯PyInstalleræ‰“åŒ…ç¯å¢ƒ
    template_folder = os.path.join(sys._MEIPASS, 'templates')
    static_folder = os.path.join(sys._MEIPASS, 'static')
    app = Flask(__name__, template_folder=template_folder, static_folder=static_folder)
else:
    app = Flask(__name__)

app.config['JSON_AS_ASCII'] = False  # æ”¯æŒä¸­æ–‡JSON

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡çŠ¶æ€
tasks = {}
tasks_lock = threading.Lock()

# ä»»åŠ¡é˜Ÿåˆ—ï¼šç”¨äºé™åˆ¶å¹¶å‘æ•°é‡
task_queue = Queue()
# æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼ˆå¯ä»¥æ ¹æ®APIé™åˆ¶è°ƒæ•´ï¼‰
MAX_CONCURRENT_TASKS = 2  # é»˜è®¤åŒæ—¶æœ€å¤šå¤„ç†2ä¸ªè§†é¢‘
# å·¥ä½œçº¿ç¨‹å¯åŠ¨æ ‡å¿—
worker_threads_started = False
worker_threads_lock = threading.Lock()


class TaskStatus:
    """ä»»åŠ¡çŠ¶æ€ç±»"""
    PENDING = "pending"
    DOWNLOADING = "downloading"
    SUMMARIZING = "summarizing"
    COMPLETED = "completed"
    FAILED = "failed"
    STOPPING = "stopping"  # æ­£åœ¨åœæ­¢ä¸­
    STOPPED = "stopped"    # å·²åœæ­¢


def load_models():
    """åŠ è½½æ¨¡å‹é…ç½®"""
    config_file = 'config/llm_models.json'
    if not os.path.exists(config_file):
        return []
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
        return config.get('models', [])


def save_models(models):
    """ä¿å­˜æ¨¡å‹é…ç½®"""
    config_file = 'config/llm_models.json'
    os.makedirs(os.path.dirname(config_file), exist_ok=True)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump({'models': models}, f, ensure_ascii=False, indent=2)


def load_app_config():
    """åŠ è½½åº”ç”¨é…ç½®"""
    config_file = 'config/app_config.json'
    
    # é»˜è®¤é…ç½®
    default_config = {
        'output_directory': 'subtitles',
        'last_selected_model': '',
        'last_workspace_name': '',
        'cookies_file': 'cookies.txt',
        'auto_refresh_interval': 2000,
        'web_port': 5000,
        'download_all_parts': False,  # é»˜è®¤å…³é—­ï¼šåªä¸‹è½½URLæŒ‡å®šçš„è§†é¢‘ï¼Œä¸ä¸‹è½½æ‰€æœ‰åˆ†P
        'max_concurrent_tasks': 2  # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼šé»˜è®¤åŒæ—¶å¤„ç†2ä¸ªè§†é¢‘ï¼ˆé¿å…APIå¹¶å‘è¿‡é«˜ï¼‰
    }
    
    if not os.path.exists(config_file):
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
        save_app_config(default_config)
        return default_config
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
            # åˆå¹¶é»˜è®¤é…ç½®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            for key, value in default_config.items():
                if key not in config:
                    config[key] = value
            return config
    except Exception as e:
        print(f"åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return default_config


def save_app_config(config):
    """ä¿å­˜åº”ç”¨é…ç½®"""
    config_file = 'config/app_config.json'
    os.makedirs(os.path.dirname(config_file), exist_ok=True)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)


def load_workspaces():
    """åŠ è½½å·¥ä½œåŒºé…ç½®"""
    config_file = 'config/workspace.json'
    if not os.path.exists(config_file):
        return []
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('workspaces', [])
    except Exception as e:
        print(f"åŠ è½½å·¥ä½œåŒºé…ç½®å¤±è´¥: {e}")
        return []


def save_workspaces(workspaces):
    """ä¿å­˜å·¥ä½œåŒºé…ç½®"""
    config_file = 'config/workspace.json'
    os.makedirs(os.path.dirname(config_file), exist_ok=True)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump({'workspaces': workspaces}, f, ensure_ascii=False, indent=2)


def start_worker_threads():
    """å¯åŠ¨å·¥ä½œçº¿ç¨‹æ± ï¼ˆç¡®ä¿åªå¯åŠ¨ä¸€æ¬¡ï¼‰"""
    global worker_threads_started
    
    with worker_threads_lock:
        if worker_threads_started:
            return
        
        # åŠ è½½é…ç½®è·å–å¹¶å‘æ•°
        config = load_app_config()
        max_concurrent = config.get('max_concurrent_tasks', MAX_CONCURRENT_TASKS)
        
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ {max_concurrent} ä¸ªå·¥ä½œçº¿ç¨‹...")
        for i in range(max_concurrent):
            worker = threading.Thread(target=task_queue_worker, daemon=True, name=f"Worker-{i+1}")
            worker.start()
        
        worker_threads_started = True
        print(f"âœ… å·¥ä½œçº¿ç¨‹æ± å·²å¯åŠ¨ï¼ˆ{max_concurrent} ä¸ªçº¿ç¨‹ï¼‰")


def save_sections_to_json(video_dir, video_url, all_generated_files):
    """
    å°†ç”Ÿæˆçš„å†…å®¹ä¿å­˜ä¸º Section JSON æ ¼å¼
    
    Args:
        video_dir: è§†é¢‘æ•°æ®ç›®å½•ï¼ˆ.../dataï¼‰
        video_url: è§†é¢‘URL
        all_generated_files: æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    import math
    
    # video_dir æŒ‡å‘ .../data ç›®å½•ï¼Œsection.json æ–‡ä»¶åœ¨ä¸Šä¸€çº§ç›®å½•
    parent_dir = os.path.dirname(video_dir)
    section_file_name = os.path.join(parent_dir, 'section.json')
    
    # è¯»å–æˆ–åˆå§‹åŒ– sections åˆ—è¡¨
    if os.path.exists(section_file_name):
        with open(section_file_name, 'r', encoding='utf-8') as f:
            sections_data = json.load(f)
    else:
        sections_data = []
    
    # å°è¯•è¯»å–è§†é¢‘ä¿¡æ¯è·å–æ—¶é•¿
    video_info = None
    video_info_files = [f for f in os.listdir(video_dir) if f.endswith('_video_info.json')]
    if video_info_files:
        video_info_path = os.path.join(video_dir, video_info_files[0])
        try:
            with open(video_info_path, 'r', encoding='utf-8') as f:
                video_info = json.load(f)
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è¯»å–è§†é¢‘ä¿¡æ¯æ–‡ä»¶: {e}")
    
    # ä¸ºæ¯ä¸ªå­—å¹•æ–‡ä»¶åˆ›å»ºä¸€ä¸ª Section å¯¹è±¡
    for file_data in all_generated_files:
        subtitle_title = file_data['subtitle_title']
        
        # è¯»å–ç»ƒä¹ é¢˜å’Œé¢„è®¾é—®é¢˜
        exercises_file = file_data.get('exercises')
        questions_file = file_data.get('questions')
        
        # è§£æç»ƒä¹ é¢˜
        exercises_list = []
        if exercises_file and os.path.exists(exercises_file):
            with open(exercises_file, 'r', encoding='utf-8') as f:
                exercises_json = json.load(f)
                
            # å¤„ç†å¤šé€‰é¢˜/å•é€‰é¢˜
            for mc in exercises_json.get("multiple_choice", []):
                options_dict = mc.get("options", {})
                correct_answer = mc.get("correct_answer", "")
                
                # è½¬æ¢é€‰é¡¹æ ¼å¼
                options_list = []
                for key in sorted(options_dict.keys()):
                    options_list.append({
                        "id": uuid.uuid4().hex,
                        "text": options_dict[key],
                        "is_correct": key in correct_answer
                    })
                
                # åˆ¤æ–­é¢˜å‹
                question_type = "å•é€‰" if len(correct_answer) == 1 else "å¤šé€‰"
                
                exercises_list.append({
                    "id": uuid.uuid4().hex,
                    "question": mc.get("question", ""),
                    "score": 5,
                    "type": question_type,
                    "options": options_list
                })
            
            # å¤„ç†ç®€ç­”é¢˜
            for sa in exercises_json.get("short_answer", []):
                reference = sa.get("reference_answer", "")
                if not reference:
                    answer_points = sa.get("answer_points", [])
                    reference = "\n".join(answer_points)
                
                exercises_list.append({
                    "id": uuid.uuid4().hex,
                    "question": sa.get("question", ""),
                    "score": 15,
                    "type": "ç®€ç­”",
                    "options": []
                })
        
        # è§£æé¢„è®¾é—®é¢˜ï¼ˆå¼•å¯¼æ€§é—®é¢˜ï¼‰
        leading_questions_list = []
        if questions_file and os.path.exists(questions_file):
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions_json = json.load(f)
                for q in questions_json.get("questions", []):
                    leading_questions_list.append({
                        "id": uuid.uuid4().hex,
                        "question": q.get("question", "")
                    })
        
        # è·å–è§†é¢‘æ—¶é•¿ï¼ˆè½¬æ¢ä¸ºåˆ†é’Ÿï¼‰
        estimated_time = 0
        if video_info:
            # å°è¯•ä» video_info è·å–æ—¶é•¿
            duration_sec = video_info.get('duration', 0)
            
            # å¦‚æœæ˜¯å¤šPè§†é¢‘ï¼Œå°è¯•åŒ¹é…å¯¹åº”çš„åˆ†Pæ—¶é•¿
            pages = video_info.get('pages', [])
            if pages:
                for page in pages:
                    page_title = page.get('part', '')
                    # å°è¯•åŒ¹é…æ ‡é¢˜ï¼ˆå»æ‰å¯èƒ½çš„å‰ç¼€ï¼‰
                    if page_title and page_title in subtitle_title:
                        duration_sec = page.get('duration', 0)
                        break
            
            # è½¬æ¢ä¸ºåˆ†é’Ÿï¼ˆå‘ä¸Šå–æ•´ï¼‰
            if duration_sec > 0:
                estimated_time = math.ceil(duration_sec / 60)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåçš„ sectionï¼ˆé€šè¿‡ title åˆ¤æ–­ï¼‰
        existing_section = None
        for section in sections_data:
            if section.get('title') == subtitle_title:
                existing_section = section
                break
        
        if existing_section:
            # æ›´æ–°ç°æœ‰ section
            existing_section['exercises'] = exercises_list
            existing_section['leading_questions'] = leading_questions_list
            existing_section['video_url'] = video_url
            existing_section['estimated_time'] = estimated_time
        else:
            # åˆ›å»ºæ–°çš„ section
            section_obj = {
                "id": uuid.uuid4().hex,
                "title": subtitle_title,
                "order": 0,  # å›ºå®šä¸º0
                "estimated_time": estimated_time,  # ä½¿ç”¨è§†é¢‘æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
                "video_url": video_url,
                "leading_questions": leading_questions_list,
                "exercises": exercises_list
            }
            sections_data.append(section_obj)
    
    # ä¿å­˜ section.json
    with open(section_file_name, 'w', encoding='utf-8') as f:
        json.dump(sections_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Sectionæ•°æ®å·²ä¿å­˜åˆ°: {section_file_name}")


def task_queue_worker():
    """ä»»åŠ¡é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹ï¼šä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¹¶æ‰§è¡Œ"""
    thread_name = threading.current_thread().name
    print(f"[{thread_name}] å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨ï¼Œç­‰å¾…ä»»åŠ¡...")
    
    while True:
        try:
            # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
            task_data = task_queue.get()
            if task_data is None:  # None æ˜¯åœæ­¢ä¿¡å·
                print(f"[{thread_name}] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º")
                break
            
            task_id = task_data['task_id']
            url = task_data['url']
            output_dir = task_data['output_dir']
            model_name = task_data['model_name']
            cookies_file = task_data['cookies_file']
            custom_folder_name = None
            download_all_parts = task_data.get('download_all_parts', False)
            generate_options = task_data.get('generate_options')
            
            print(f"[{thread_name}] å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}: {url}")
            
            # æ‰§è¡Œä»»åŠ¡
            process_video_task(task_id, thread_name, url, output_dir, model_name, cookies_file, custom_folder_name, download_all_parts, generate_options)
            
            print(f"[{thread_name}] ä»»åŠ¡ {task_id} å¤„ç†å®Œæˆ")
            
            # ä»»åŠ¡å®Œæˆåç­‰å¾…ä¸€å°æ®µæ—¶é—´å†å¤„ç†ä¸‹ä¸€ä¸ªï¼ˆé¿å…è§¦å‘åçˆ¬è™«ï¼‰
            time.sleep(2)
            
        except Exception as e:
            print(f"[{thread_name}] ä»»åŠ¡é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            task_queue.task_done()


def process_video_task(task_id, thread_name, url, output_dir, model_name, cookies_file, custom_folder_name=None, download_all_parts=False, generate_options=None):
    """å¤„ç†å•ä¸ªè§†é¢‘çš„ä¸‹è½½å’Œæ€»ç»“ä»»åŠ¡"""
    if generate_options is None:
        generate_options = {
            'summary': True,
            'full_content': True,
            'exercises': True,
            'questions': True
        }
    try:
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        with tasks_lock:
            if tasks[task_id].get('stop_flag'):
                tasks[task_id]['status'] = TaskStatus.STOPPED
                tasks[task_id]['message'] = 'ä»»åŠ¡å·²åœæ­¢'
                return
        
        # æ›´æ–°çŠ¶æ€ï¼šä¸‹è½½ä¸­
        with tasks_lock:
            tasks[task_id]['status'] = TaskStatus.DOWNLOADING
            tasks[task_id]['message'] = f'æ­£åœ¨ä¸‹è½½å­—å¹•: {url}'
        
        # åŠ è½½Cookie
        config_cookies = load_cookies_from_file(cookies_file)
        sessdata = config_cookies.get('sessdata')
        
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = BilibiliSubtitleDownloader(
            sessdata=sessdata,
            bili_jct=config_cookies.get('bili_jct'),
            buvid3=config_cookies.get('buvid3'),
            debug=False
        )
        
        # ä¸‹è½½å­—å¹•å’Œå°é¢
        download_result = downloader.download(
            video_url=url,
            video_index=thread_name,
            output_dir=output_dir,
            format_type='srt',
            download_cover=True,
            custom_folder_name=custom_folder_name,
            download_all_parts=download_all_parts
        )
        
        # è·å–ä¸‹è½½ç»“æœ
        downloaded_files = download_result.get('subtitles', [])
        cover_path = download_result.get('cover')
        video_title = download_result.get('title', '')
        video_dir = download_result.get('video_dir', output_dir)
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸä¸‹è½½äº†å­—å¹•
        if not downloaded_files:
            raise Exception('æ­¤è§†é¢‘æ²¡æœ‰å­—å¹•ï¼Œæ— æ³•è¿›è¡Œæ€»ç»“')
        
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        with tasks_lock:
            if tasks[task_id].get('stop_flag'):
                tasks[task_id]['status'] = TaskStatus.STOPPED
                tasks[task_id]['message'] = 'ä»»åŠ¡å·²åœæ­¢'
                return
        
        # æ›´æ–°ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
        with tasks_lock:
            tasks[task_id]['video_dir'] = video_dir
            tasks[task_id]['video_title'] = video_title
        
        # åŠ è½½LLMé…ç½®
        llm_config_file = 'config/llm_models.json'
        model_config = load_llm_config(llm_config_file, model_name=model_name)
        
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm_client = OpenAICompatClient(
            api_base=model_config['api_base'],
            api_key=model_config['api_key'],
            default_model=model_config['model_name'],
            request_timeout=500
        )
        
        # åˆ›å»ºæ€»ç»“å™¨
        summarizer = SubtitleSummarizer(llm_client)
        
        # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶
        all_generated_files = []
        
        # éå†æ‰€æœ‰ä¸‹è½½çš„ä¸­æ–‡å­—å¹•æ–‡ä»¶
        total_files = len(downloaded_files)
        for file_index, subtitle_file in enumerate(downloaded_files, 1):
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            with tasks_lock:
                if tasks[task_id].get('stop_flag'):
                    tasks[task_id]['status'] = TaskStatus.STOPPED
                    tasks[task_id]['message'] = 'ä»»åŠ¡å·²åœæ­¢'
                    return
            
            # ä»å­—å¹•æ–‡ä»¶åä¸­æå–æ ‡é¢˜
            subtitle_filename = os.path.basename(subtitle_file)
            subtitle_name = os.path.splitext(subtitle_filename)[0]
            subtitle_title = subtitle_name.rsplit('_', 1)[0] if '_' in subtitle_name else subtitle_name
            
            # å®šä¹‰æ‰€æœ‰å¯èƒ½ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„
            summary_json_file = os.path.join(video_dir, f'{subtitle_title}_summary.json')
            # markdown_dir = os.path.join(video_dir, 'markdown')
            # full_content_file = os.path.join(markdown_dir, f'{subtitle_title}.md')
            # æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼ŒMarkdownæ–‡ä»¶åº”ç›´æ¥å­˜æ”¾åœ¨video_dirä¸‹
            full_content_file = os.path.join(video_dir, f'{subtitle_title}.md')
            exercises_file = os.path.join(video_dir, f'{subtitle_title}_exercises.json')
            questions_file = os.path.join(video_dir, f'{subtitle_title}_questions.json')
            
            # è§£æå­—å¹•ï¼ˆæå‰è§£æï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨ï¼‰
            subtitles = None
            subtitle_text = None
            plain_text = None
            
            # ========== 1. ç”Ÿæˆè¦ç‚¹æ€»ç»“ ==========
            if generate_options.get('summary', True):
                if os.path.exists(summary_json_file):
                    with tasks_lock:
                        tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (1/4): è¦ç‚¹æ€»ç»“å·²å­˜åœ¨ï¼Œè·³è¿‡'
                else:
                    with tasks_lock:
                        tasks[task_id]['status'] = TaskStatus.SUMMARIZING
                        tasks[task_id]['message'] = f'æ­£åœ¨å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (1/4): è¦ç‚¹æ€»ç»“...'
                        tasks[task_id]['subtitle_file'] = subtitle_file
                    
                    # è§£æå­—å¹•
                    if subtitles is None:
                        subtitles = SRTParser.parse_srt_file(subtitle_file)
                        subtitle_text = SRTParser.format_subtitles_for_llm(subtitles)
                    
                    summary = summarizer.summarize(subtitle_text, stream=False)
                    
                    with open(summary_json_file, 'w', encoding='utf-8') as f:
                        json.dump(summary, f, ensure_ascii=False, indent=2)
            else:
                with tasks_lock:
                     tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (1/4): è¦ç‚¹æ€»ç»“ (ç”¨æˆ·é€‰æ‹©è·³è¿‡)'
            
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            with tasks_lock:
                if tasks[task_id].get('stop_flag'):
                    tasks[task_id]['status'] = TaskStatus.STOPPED
                    tasks[task_id]['message'] = 'ä»»åŠ¡å·²åœæ­¢'
                    return
            
            # ========== 2. ç”Ÿæˆå®Œæ•´å†…å®¹æ–‡æ¡£ ==========
            if generate_options.get('full_content', True):
                if os.path.exists(full_content_file):
                    with tasks_lock:
                        tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (2/4): å®Œæ•´æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡'
                else:
                    with tasks_lock:
                        tasks[task_id]['message'] = f'æ­£åœ¨å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (2/4): å®Œæ•´æ–‡æ¡£...'
                    
                    # é¢„å¤„ç†å­—å¹•æ–‡æœ¬
                    if plain_text is None:
                        plain_text = SRTParser.extract_plain_text(subtitle_file)
                    
                    full_content = summarizer.generate_full_content(
                        plain_text,
                        video_title=video_title,
                        stream=False
                    )
                    
                    # åˆ›å»ºmarkdownå­ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                    # os.makedirs(markdown_dir, exist_ok=True)
                    
                    with open(full_content_file, 'w', encoding='utf-8') as f:
                        f.write(full_content)
            else:
                with tasks_lock:
                     tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (2/4): å®Œæ•´æ–‡æ¡£ (ç”¨æˆ·é€‰æ‹©è·³è¿‡)'
            
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            with tasks_lock:
                if tasks[task_id].get('stop_flag'):
                    tasks[task_id]['status'] = TaskStatus.STOPPED
                    tasks[task_id]['message'] = 'ä»»åŠ¡å·²åœæ­¢'
                    return
            
            # ========== 3. ç”Ÿæˆç»ƒä¹ é¢˜ ==========
            if generate_options.get('exercises', True):
                if os.path.exists(exercises_file):
                    with tasks_lock:
                        tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (3/4): ç»ƒä¹ é¢˜å·²å­˜åœ¨ï¼Œè·³è¿‡'
                else:
                    with tasks_lock:
                        tasks[task_id]['message'] = f'æ­£åœ¨å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (3/4): ç»ƒä¹ é¢˜...'
                    
                    # é¢„å¤„ç†å­—å¹•æ–‡æœ¬ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                    if plain_text is None:
                        plain_text = SRTParser.extract_plain_text(subtitle_file)
                    
                    exercises = summarizer.generate_exercises(
                        plain_text,
                        video_title=video_title,
                        stream=False
                    )
                    
                    with open(exercises_file, 'w', encoding='utf-8') as f:
                        json.dump(exercises, f, ensure_ascii=False, indent=2)
            else:
                with tasks_lock:
                     tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (3/4): ç»ƒä¹ é¢˜ (ç”¨æˆ·é€‰æ‹©è·³è¿‡)'
            
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            with tasks_lock:
                if tasks[task_id].get('stop_flag'):
                    tasks[task_id]['status'] = TaskStatus.STOPPED
                    tasks[task_id]['message'] = 'ä»»åŠ¡å·²åœæ­¢'
                    return
            
            # ========== 4. ç”Ÿæˆé¢„è®¾é—®é¢˜ ==========
            if generate_options.get('questions', True):
                if os.path.exists(questions_file):
                    with tasks_lock:
                        tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (4/4): é¢„è®¾é—®é¢˜å·²å­˜åœ¨ï¼Œè·³è¿‡'
                else:
                    with tasks_lock:
                        tasks[task_id]['message'] = f'æ­£åœ¨å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (4/4): é¢„è®¾é—®é¢˜...'
                    
                    # é¢„å¤„ç†å­—å¹•æ–‡æœ¬ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                    if plain_text is None:
                        plain_text = SRTParser.extract_plain_text(subtitle_file)
                    
                    preset_questions = summarizer.generate_preset_questions(
                        plain_text,
                        video_title=video_title,
                        stream=False
                    )
                    
                    with open(questions_file, 'w', encoding='utf-8') as f:
                        json.dump(preset_questions, f, ensure_ascii=False, indent=2)
            else:
                with tasks_lock:
                     tasks[task_id]['message'] = f'å¤„ç†å­—å¹• {file_index}/{total_files}: {subtitle_title} (4/4): é¢„è®¾é—®é¢˜ (ç”¨æˆ·é€‰æ‹©è·³è¿‡)'
            
            # è®°å½•æœ¬å­—å¹•ç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶
            all_generated_files.append({
                'subtitle_title': subtitle_title,
                'subtitle_file': subtitle_file,
                'summary_json': summary_json_file,
                'content_md': full_content_file,
                'exercises': exercises_file,
                'questions': questions_file
            })
        # 5. å°†ç”Ÿæˆçš„æ•°æ®å†™å…¥ JSON
        with tasks_lock:
            tasks[task_id]['message'] = f'æ­£åœ¨å†™å…¥Sectionæ•°æ®: {video_title}...'
        
        save_sections_to_json(video_dir, url, all_generated_files)
        

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        with tasks_lock:
            tasks[task_id]['status'] = TaskStatus.COMPLETED
            tasks[task_id]['message'] = f'å…¨éƒ¨å®Œæˆï¼å·²å¤„ç† {total_files} ä¸ªå­—å¹•æ–‡ä»¶ï¼Œç”Ÿæˆäº†å­—å¹•ã€å°é¢ã€æ€»ç»“ã€å®Œæ•´æ–‡æ¡£ã€ç»ƒä¹ é¢˜å’Œé¢„è®¾é—®é¢˜'
            tasks[task_id]['files'] = {
                'video_dir': video_dir,
                'cover': cover_path,
                'generated_files': all_generated_files
            }
            tasks[task_id]['completed_at'] = datetime.now().isoformat()
        
    except Exception as e:
        # æ›´æ–°çŠ¶æ€ï¼šå¤±è´¥
        with tasks_lock:
            tasks[task_id]['status'] = TaskStatus.FAILED
            tasks[task_id]['message'] = f'é”™è¯¯: {str(e)}'
            tasks[task_id]['error'] = str(e)




# ==================== Webè·¯ç”± ====================

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')


@app.route('/api/models', methods=['GET'])
def get_models():
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    try:
        models = load_models()
        return jsonify({
            'success': True,
            'models': models
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/models', methods=['POST'])
def add_model():
    """æ·»åŠ æ¨¡å‹"""
    try:
        data = request.json
        
        # ç”Ÿæˆå”¯ä¸€ID
        model_id = datetime.now().strftime('%Y%m%d%H%M%S%f')
        
        new_model = {
            'id': model_id,
            'name': data.get('name'),
            'model_name': data.get('model_name'),
            'api_base': data.get('api_base'),
            'api_key': data.get('api_key')
        }
        
        models = load_models()
        models.append(new_model)
        save_models(models)
        
        return jsonify({
            'success': True,
            'model': new_model
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/models/<model_id>', methods=['PUT'])
def update_model(model_id):
    """æ›´æ–°æ¨¡å‹"""
    try:
        data = request.json
        models = load_models()
        
        for model in models:
            if model['id'] == model_id:
                model['name'] = data.get('name', model['name'])
                model['model_name'] = data.get('model_name', model['model_name'])
                model['api_base'] = data.get('api_base', model['api_base'])
                model['api_key'] = data.get('api_key', model['api_key'])
                break
        
        save_models(models)
        
        return jsonify({
            'success': True
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/models/<model_id>', methods=['DELETE'])
def delete_model(model_id):
    """åˆ é™¤æ¨¡å‹"""
    try:
        models = load_models()
        models = [m for m in models if m['id'] != model_id]
        save_models(models)
        
        return jsonify({
            'success': True
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/workspace', methods=['POST'])
def create_workspace():
    """åˆ›å»ºå·¥ä½œåŒº"""
    try:
        data = request.json
        name = data.get('name', '').strip()
        path = str(Path(data.get('path', '').strip()))
        
        if not name:
            return jsonify({
                'success': False,
                'error': 'å·¥ä½œåŒºåç§°ä¸èƒ½ä¸ºç©º'
            }), 400
        
        if not path:
            return jsonify({
                'success': False,
                'error': 'å·¥ä½œåŒºè·¯å¾„ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # åŠ è½½ç°æœ‰å·¥ä½œåŒº
        workspaces = load_workspaces()
        
        # æ£€æŸ¥åç§°æ˜¯å¦é‡å¤
        for ws in workspaces:
            if ws.get('name') == name:
                return jsonify({
                    'success': False,
                    'error': f'å·¥ä½œåŒºåç§° "{name}" å·²å­˜åœ¨'
                }), 400
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        full_path = os.path.abspath(path)
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(full_path):
            return jsonify({
                'success': False,
                'error': f'è·¯å¾„ "{path}" å·²å­˜åœ¨'
            }), 400
        
        # åˆ›å»ºæ–‡ä»¶å¤¹
        try:
            os.makedirs(full_path, exist_ok=False)
        except OSError as e:
            return jsonify({
                'success': False,
                'error': f'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {str(e)}'
            }), 500
        
        # åˆ›å»º course.json æ–‡ä»¶
        try:
            course_data = create_empty_course(title=name)
            course_file_path = os.path.join(full_path, 'course.json')
            with open(course_file_path, 'w', encoding='utf-8') as f:
                json.dump(course_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            # å¦‚æœåˆ›å»º course.json å¤±è´¥ï¼Œæ¸…ç†å·²åˆ›å»ºçš„æ–‡ä»¶å¤¹
            try:
                os.rmdir(full_path)
            except:
                pass
            return jsonify({
                'success': False,
                'error': f'åˆ›å»ºè¯¾ç¨‹æ–‡ä»¶å¤±è´¥: {str(e)}'
            }), 500
        
        # æ·»åŠ æ–°å·¥ä½œåŒº
        new_workspace = {
            'name': name,
            'path': path,
            'created_at': datetime.now().isoformat()
        }
        workspaces.append(new_workspace)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_workspaces(workspaces)
        
        return jsonify({
            'success': True,
            'workspace': new_workspace,
            'full_path': full_path
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/workspace', methods=['GET'])
def list_workspaces():
    """è·å–æ‰€æœ‰å·¥ä½œåŒºåˆ—è¡¨"""
    try:
        workspaces = load_workspaces()
        return jsonify({
            'success': True,
            'workspaces': workspaces
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/workspace/<workspace_name>', methods=['DELETE'])
def delete_workspace(workspace_name):
    """åˆ é™¤å·¥ä½œåŒº"""
    try:
        # åŠ è½½ç°æœ‰å·¥ä½œåŒº
        workspaces = load_workspaces()
        
        # æŸ¥æ‰¾è¦åˆ é™¤çš„å·¥ä½œåŒº
        target_workspace = None
        for ws in workspaces:
            if ws.get('name') == workspace_name:
                target_workspace = ws
                break
        
        if not target_workspace:
            return jsonify({
                'success': False,
                'error': f'å·¥ä½œåŒº "{workspace_name}" ä¸å­˜åœ¨'
            }), 404
        
        # è·å–å·¥ä½œåŒºè·¯å¾„
        workspace_path = target_workspace.get('path')
        full_path = os.path.abspath(workspace_path)
        
        # åˆ é™¤æ–‡ä»¶å¤¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(full_path):
            try:
                shutil.rmtree(full_path)
            except OSError as e:
                return jsonify({
                    'success': False,
                    'error': f'åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}'
                }), 500
        
        # ä»åˆ—è¡¨ä¸­åˆ é™¤å·¥ä½œåŒº
        workspaces = [ws for ws in workspaces if ws.get('name') != workspace_name]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_workspaces(workspaces)
        
        return jsonify({
            'success': True,
            'message': f'å·¥ä½œåŒº "{workspace_name}" å·²åˆ é™¤',
            'deleted_path': full_path
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/tasks', methods=['POST'])
def create_tasks():
    """åˆ›å»ºæ‰¹é‡ä»»åŠ¡"""
    try:
        # ç¡®ä¿å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨
        start_worker_threads()
        
        data = request.json
        urls = data.get('urls', [])
        workspace_name = data.get('workspace_name')
        output_dir = None
        for ws in load_workspaces():
            if ws.get('name') == workspace_name:
                output_dir = ws.get('path')
                break

        if output_dir is None:
            return jsonify({
                'success': False,
                'error': 'å·¥ä½œåŒºä¸å­˜åœ¨'
            }), 400
            
        model_name = data.get('model_name')
        cookies_file = data.get('cookies_file', 'cookies.txt')
        download_all_parts = data.get('download_all_parts', False)
        generate_options = data.get('generate_options', {
            'summary': True,
            'full_content': True,
            'exercises': True,
            'questions': True
        })
        
        if not urls:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›è‡³å°‘ä¸€ä¸ªURL'
            }), 400
        
        if not model_name:
            return jsonify({
                'success': False,
                'error': 'è¯·é€‰æ‹©æ¨¡å‹'
            }), 400
        
        # ä¿å­˜é…ç½®ï¼šæœ€åä½¿ç”¨çš„è¾“å‡ºç›®å½•å’Œæ¨¡å‹
        config = load_app_config()
        config['last_workspace_name'] = workspace_name
        config['last_selected_model'] = model_name
        save_app_config(config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½Cookieç”¨äºæ£€æµ‹æ”¶è—å¤¹
        config_cookies = load_cookies_from_file(cookies_file)
        
        # å¤„ç†URLåˆ—è¡¨ï¼Œå±•å¼€æ”¶è—å¤¹URL
        expanded_urls = []
        for url in urls:
            url = url.strip()
            if not url:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¶è—å¤¹URL
            downloader = BilibiliSubtitleDownloader(
                sessdata=config_cookies.get('sessdata'),
                bili_jct=config_cookies.get('bili_jct'),
                buvid3=config_cookies.get('buvid3'),
                debug=False
            )
            
            if downloader.is_favorite_url(url):
                # è·å–æ”¶è—å¤¹ID
                fid = downloader.extract_fid(url)
                if fid:
                    print(f"æ£€æµ‹åˆ°æ”¶è—å¤¹URLï¼Œæ­£åœ¨è·å–è§†é¢‘åˆ—è¡¨...")
                    # è·å–æ”¶è—å¤¹å†…çš„è§†é¢‘åˆ—è¡¨
                    videos = downloader.get_favorite_videos(fid)
                    for video in videos:
                        video_url = f"https://www.bilibili.com/video/{video['bvid']}"
                        expanded_urls.append(video_url)
                    print(f"æ”¶è—å¤¹å±•å¼€å®Œæˆï¼Œå…± {len(videos)} ä¸ªè§†é¢‘")
                else:
                    # æ— æ³•æå–æ”¶è—å¤¹IDï¼Œå½“ä½œæ™®é€šè§†é¢‘URLå¤„ç†
                    print(f"æ— æ³•ä»æ”¶è—å¤¹URLä¸­æå–IDï¼Œå°†ä½œä¸ºæ™®é€šè§†é¢‘å¤„ç†: {url}")
                    expanded_urls.append(url)
            else:
                # æ™®é€šè§†é¢‘URL
                expanded_urls.append(url)
        
        if not expanded_urls:
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘URL'
            }), 400
        
        # åˆ›å»ºä»»åŠ¡å¹¶åŠ å…¥é˜Ÿåˆ—
        task_ids = []
        for index, url in enumerate(expanded_urls):
            task_id = str(uuid.uuid4())
            
            with tasks_lock:
                tasks[task_id] = {
                    'id': task_id,
                    'url': url,
                    'status': TaskStatus.PENDING,
                    'message': 'ç­‰å¾…é˜Ÿåˆ—å¤„ç†ï¼ˆé¿å…å¹¶å‘è¿‡é«˜ï¼‰',
                    'created_at': datetime.now().isoformat()
                }
            
            # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—ï¼Œè€Œä¸æ˜¯ç›´æ¥å¯åŠ¨çº¿ç¨‹
            task_data = {
                'task_id': task_id,
                'url': url,
                'output_dir': output_dir,
                'model_name': model_name,
                'cookies_file': cookies_file,
                'download_all_parts': download_all_parts,
                'generate_options': generate_options
            }
            task_queue.put(task_data)
            
            task_ids.append(task_id)
        
        # è·å–å½“å‰é…ç½®çš„å¹¶å‘æ•°
        config = load_app_config() 
        max_concurrent = config.get('max_concurrent_tasks', MAX_CONCURRENT_TASKS)
        
        return jsonify({
            'success': True,
            'task_ids': task_ids,
            'total_videos': len(expanded_urls),
            'message': f'å·²åˆ›å»º {len(task_ids)} ä¸ªä»»åŠ¡ï¼Œæœ€å¤šåŒæ—¶å¤„ç† {max_concurrent} ä¸ªè§†é¢‘'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/tasks/<task_id>', methods=['GET'])
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    with tasks_lock:
        task = tasks.get(task_id)
        if not task:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
            }), 404
        
        return jsonify({
            'success': True,
            'task': task
        })


@app.route('/api/tasks', methods=['GET'])
def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡"""
    with tasks_lock:
        return jsonify({
            'success': True,
            'tasks': list(tasks.values())
        })


@app.route('/api/tasks/<task_id>/stop', methods=['POST'])
def stop_task(task_id):
    """åœæ­¢ä»»åŠ¡"""
    try:
        with tasks_lock:
            task = tasks.get(task_id)
            if not task:
                return jsonify({
                    'success': False,
                    'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
                }), 404
            
            # å¦‚æœä»»åŠ¡å·²ç»å®Œæˆã€å¤±è´¥æˆ–åœæ­¢ï¼Œåˆ™ä¸èƒ½å†åœæ­¢
            if task['status'] in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.STOPPED]:
                return jsonify({
                    'success': False,
                    'error': f'ä»»åŠ¡å·²ç»æ˜¯{task["status"]}çŠ¶æ€ï¼Œæ— æ³•åœæ­¢'
                }), 400
            
            # è®¾ç½®åœæ­¢æ ‡å¿—å’Œæ›´æ–°çŠ¶æ€
            task['stop_flag'] = True
            task['status'] = TaskStatus.STOPPING
            task['message'] = 'æ­£åœ¨åœæ­¢ä»»åŠ¡ï¼Œè¯·ç­‰å¾…å½“å‰æ­¥éª¤å®Œæˆ...'
        
        return jsonify({
            'success': True,
            'message': 'åœæ­¢ä¿¡å·å·²å‘é€'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/config/cookies', methods=['GET'])
def get_cookies_config():
    """è·å–Cookieé…ç½®"""
    try:
        cookies_file = 'cookies.txt'
        if not os.path.exists(cookies_file):
            return jsonify({
                'success': True,
                'configured': False
            })
        
        cookies = load_cookies_from_file(cookies_file)
        return jsonify({
            'success': True,
            'configured': bool(cookies.get('sessdata'))
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/config', methods=['GET'])
def get_app_config():
    """è·å–åº”ç”¨é…ç½®"""
    try:
        config = load_app_config()
        return jsonify({
            'success': True,
            'config': config
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/config', methods=['POST'])
def update_app_config():
    """æ›´æ–°åº”ç”¨é…ç½®"""
    try:
        data = request.json
        
        # åŠ è½½å½“å‰é…ç½®
        config = load_app_config()
        
        # æ›´æ–°é…ç½®å­—æ®µ
        if 'output_directory' in data:
            config['output_directory'] = data['output_directory']
        if 'last_selected_model' in data:
            config['last_selected_model'] = data['last_selected_model']
        if 'cookies_file' in data:
            config['cookies_file'] = data['cookies_file']
        if 'auto_refresh_interval' in data:
            config['auto_refresh_interval'] = data['auto_refresh_interval']
        if 'web_port' in data:
            config['web_port'] = data['web_port']
        if 'download_all_parts' in data:
            config['download_all_parts'] = data['download_all_parts']
        if 'max_concurrent_tasks' in data:
            config['max_concurrent_tasks'] = data['max_concurrent_tasks']
        
        # ä¿å­˜é…ç½®
        save_app_config(config)
        
        return jsonify({
            'success': True,
            'config': config
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/course/<workspace_name>', methods=['GET'])
def get_course_with_sections(workspace_name):
    """è·å–æŒ‡å®šå·¥ä½œåŒºçš„course.jsonå’Œæ‰€æœ‰section.jsonæ–‡ä»¶"""
    try:
        # æŸ¥æ‰¾å·¥ä½œåŒº
        workspaces = load_workspaces()
        workspace = None
        for ws in workspaces:
            if ws.get('name') == workspace_name:
                workspace = ws
                break
        
        if not workspace:
            return jsonify({
                'success': False,
                'error': f'å·¥ä½œåŒº "{workspace_name}" ä¸å­˜åœ¨'
            }), 404
        
        workspace_path = os.path.abspath(workspace.get('path'))
        
        # æ£€æŸ¥å·¥ä½œåŒºç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(workspace_path):
            return jsonify({
                'success': False,
                'error': f'å·¥ä½œåŒºè·¯å¾„ "{workspace_path}" ä¸å­˜åœ¨'
            }), 404
        
        # è¯»å–course.json
        course_file = os.path.join(workspace_path, 'course.json')
        if not os.path.exists(course_file):
            return jsonify({
                'success': False,
                'error': 'course.json æ–‡ä»¶ä¸å­˜åœ¨'
            }), 404
        
        with open(course_file, 'r', encoding='utf-8') as f:
            course_data = json.load(f)
        
        # æŸ¥æ‰¾æ‰€æœ‰section.jsonæ–‡ä»¶
        sections = []
        for root, dirs, files in os.walk(workspace_path):
            if 'section.json' in files:
                section_file = os.path.join(root, 'section.json')
                try:
                    with open(section_file, 'r', encoding='utf-8') as f:
                        section_data = json.load(f)
                        # section.jsonåŒ…å«çš„æ˜¯æ•°ç»„ï¼Œå°†æ•°ç»„å…ƒç´ ç›´æ¥æ·»åŠ åˆ°sectionsä¸­
                        if isinstance(section_data, list):
                            sections.extend(section_data)
                        else:
                            # å¦‚æœä¸æ˜¯æ•°ç»„ï¼Œä½œä¸ºå•ä¸ªå…ƒç´ æ·»åŠ 
                            sections.append(section_data)
                except Exception as e:
                    print(f"è¯»å–section.jsonæ–‡ä»¶å¤±è´¥: {section_file}, é”™è¯¯: {e}")
        
        return jsonify({
            'success': True,
            'course': course_data,
            'sections': sections
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/course/<workspace_name>', methods=['POST'])
def save_course(workspace_name):
    """ä¿å­˜course.jsonæ–‡ä»¶ï¼ˆè¦†ç›–åŸæœ‰æ–‡ä»¶ï¼‰"""
    try:
        # æŸ¥æ‰¾å·¥ä½œåŒº
        workspaces = load_workspaces()
        workspace = None
        for ws in workspaces:
            if ws.get('name') == workspace_name:
                workspace = ws
                break
        
        if not workspace:
            return jsonify({
                'success': False,
                'error': f'å·¥ä½œåŒº "{workspace_name}" ä¸å­˜åœ¨'
            }), 404
        
        workspace_path = os.path.abspath(workspace.get('path'))
        
        # æ£€æŸ¥å·¥ä½œåŒºç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(workspace_path):
            return jsonify({
                'success': False,
                'error': f'å·¥ä½œåŒºè·¯å¾„ "{workspace_path}" ä¸å­˜åœ¨'
            }), 404
        
        # è·å–å‰ç«¯æäº¤çš„courseæ•°æ®
        data = request.json
        course_data = data.get('course')
        
        if not course_data:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘courseæ•°æ®'
            }), 400
        
        # ä¿å­˜course.jsonæ–‡ä»¶ï¼ˆè¦†ç›–åŸæœ‰æ–‡ä»¶ï¼‰
        course_file = os.path.join(workspace_path, 'course.json')
        with open(course_file, 'w', encoding='utf-8') as f:
            json.dump(course_data, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            'success': True,
            'message': 'course.json å·²æˆåŠŸä¿å­˜'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    os.makedirs('config', exist_ok=True)
    os.makedirs('subtitles', exist_ok=True)
    os.makedirs('templates', exist_ok=True)
    
    # åŠ è½½é…ç½®è·å–ç«¯å£å’Œå¹¶å‘æ•°
    config = load_app_config()
    port = config.get('web_port', 5000)
    max_concurrent = config.get('max_concurrent_tasks', MAX_CONCURRENT_TASKS)
    
    print("=" * 80)
    print("Bilibiliè§†é¢‘å­—å¹•ä¸‹è½½ä¸æ€»ç»“ - WebæœåŠ¡")
    print("=" * 80)
    print()
    print("æœåŠ¡å¯åŠ¨ä¸­...")
    print(f"è®¿é—®åœ°å€: http://127.0.0.1:{port}")
    print(f"æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent}")
    print(f"å·¥ä½œçº¿ç¨‹å°†åœ¨é¦–æ¬¡åˆ›å»ºä»»åŠ¡æ—¶å¯åŠ¨")
    print()
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 80)
    print()
    
    # ä½¿ç”¨ use_reloader=False é¿å…Flaské‡æ–°åŠ è½½å¯¼è‡´å·¥ä½œçº¿ç¨‹ä¸¢å¤±
    # ä½¿ç”¨ debug=False åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ–è€…ç¡®ä¿å·¥ä½œçº¿ç¨‹åœ¨æ¯æ¬¡é‡è½½åéƒ½èƒ½æ­£ç¡®å¯åŠ¨
    app.run(host='0.0.0.0', port=port, debug=True, use_reloader=False, threaded=True)



